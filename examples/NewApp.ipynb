{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76bf7b05-cb37-4b63-99e0-15b2b8215177",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'astunparse'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mload_ext\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mautoreload\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mautoreload\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m2\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmetanno\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrecipes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mner\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NERApp\n\u001B[1;32m      6\u001B[0m colors \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb(255,200,206)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb(210,236,247)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb(250,212,229)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     16\u001B[0m ]\n",
      "File \u001B[0;32m~/Development/metanno/metanno/__init__.py:1\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmanager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AppManager\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m App\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m kernel_only, frontend_only, chain_map, chain_list, produce, get_idx\n",
      "File \u001B[0;32m~/Development/metanno/metanno/manager.py:9\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mipykernel\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Comm\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimmutable\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoProxy, apply_patches, commit\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython_to_javascript\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transcrypt_class\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mviews\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SpanEditor, TableEditor\n\u001B[1;32m     12\u001B[0m val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Development/metanno/metanno/python_to_javascript.py:15\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m defaultdict\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mast\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mastunparse\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mweakmethod\u001B[39;00m:\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, cls_method):\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'astunparse'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from metanno.recipes.ner import NERApp\n",
    "\n",
    "colors = [\n",
    "    \"rgb(255,200,206)\",\n",
    "    \"rgb(210,236,247)\",\n",
    "    \"rgb(211,242,206)\",\n",
    "    \"rgb(242,242,206)\",\n",
    "    \"rgb(231,210,247)\",\n",
    "    \"rgb(252,215,216)\",\n",
    "    \"rgb(251,243,219)\",\n",
    "    \"rgb(250,231,212)\",\n",
    "    \"rgb(250,212,229)\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e661f14-c21c-4ef9-a042-35cb1a2b20b6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<edsnlp.pipelines.qualifiers.negation.negation.Negation at 0x7f8e68d9d610>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank('eds')\n",
    "nlp.add_pipe(\"eds.normalizer\")\n",
    "nlp.add_pipe(\"eds.sentences\")\n",
    "\n",
    "# Entity extraction pipelines\n",
    "nlp.add_pipe(\"eds.covid\")\n",
    "nlp.add_pipe(\"eds.dates\")\n",
    "nlp.add_pipe(\"eds.measures\")\n",
    "nlp.add_pipe(\"eds.charlson\")\n",
    "nlp.add_pipe(\"eds.SOFA\")\n",
    "nlp.add_pipe(\"eds.emergency.priority\")\n",
    "\n",
    "nlp.add_pipe(\n",
    "    \"eds.matcher\",\n",
    "    config=dict(\n",
    "        regex=dict(custom=r\"texte|asthmatique|difficult[ée]s?\\srespiratoires?\"),\n",
    "        attr=\"NORM\",\n",
    "    ),\n",
    ")\n",
    "nlp.add_pipe(\"eds.negation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790e60b5-2ecd-4109-afae-235d8aff5bd6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"\"\"Motif :\n",
    "Le patient est admis le 29 août pour des difficultés respiratoires.\n",
    "\n",
    "Antécédents familiaux :\n",
    "Le père du patient n'est pas asthmatique.\n",
    "\n",
    "HISTOIRE DE LA MALADIE\n",
    "Le patient dit avoir de la toux depuis trois jours. Elle a empiré jusqu'à nécessiter un passage aux urgences.\n",
    "A noter deux petits kystes bénins de 1 et 2cm biopsiés en 2005.\n",
    "\n",
    "Priorité: 2 (établie par l'IAO à l'entrée)\n",
    "\n",
    "Conclusion\n",
    "Possible infection au coronavirus\n",
    "\"\"\" * 5,\n",
    "    \"\"\"Ceci n'est pas mon texte.\"\"\",\n",
    "]\n",
    "docs = []\n",
    "for i, doc in enumerate(nlp.pipe(texts)):\n",
    "    entry = {\"id\": f\"{i}\", \"text\": str(doc), \"entities\": []}\n",
    "    for i, ent in enumerate(doc.ents):\n",
    "        entry[\"entities\"].append({\n",
    "            \"id\": i,\n",
    "            \"begin\": ent.start_char,\n",
    "            \"end\": ent.end_char,\n",
    "            \"label\": ent.label_,\n",
    "            \"negation\": ent._.negation,\n",
    "            \"suggestions\": {\n",
    "                \"concept\": [\"C9876\"],\n",
    "                \"label\": [\"\"]\n",
    "            }\n",
    "        })\n",
    "    docs.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3cce044-a80e-4d87-a08c-ad63cc6c2de7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pathlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46a7e67f-36b3-4476-9880-b625e1dfd842",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EDSDataConnector:\n",
    "    # TODO, fix this with Proxy states\n",
    "    def __init__(self, path):\n",
    "        self.path = pathlib.Path(path)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        \n",
    "    def load_one(self, filename):\n",
    "        # Open the raw text file\n",
    "        try:\n",
    "            text_filepath = self.path.joinpath(filename)\n",
    "            with open(text_filepath) as f:\n",
    "                text = f.read()\n",
    "        except:\n",
    "            return\n",
    "            \n",
    "        # Open the annotations if any\n",
    "        json_filepath = self.path.joinpath(f\"{filename}.json\")\n",
    "        if os.path.exists(json_filepath):\n",
    "            with open(json_filepath) as f:\n",
    "                doc = json.load(f)\n",
    "                doc[\"id\"] = str(filename)\n",
    "                doc[\"text\"] = text\n",
    "        else:\n",
    "            doc = {\"id\": str(filename), \"text\": text, \"entities\": []}\n",
    "            \n",
    "        return doc\n",
    "            \n",
    "    def load(self):\n",
    "        filenames = list(self.path.rglob(\"*.txt\"))\n",
    "        docs = []\n",
    "        for filename in sorted(filenames):\n",
    "            if \".ipynb_checkpoints\" in str(filename):\n",
    "                continue\n",
    "            filename = pathlib.Path(filename).relative_to(self.path)\n",
    "            doc = self.load_one(filename)\n",
    "            if doc is not None:\n",
    "                docs.append(doc)\n",
    "        return docs\n",
    "    \n",
    "    def save_one(self, doc):\n",
    "        with open(self.path.joinpath(f\"{doc['id']}.json\"), \"w\") as f:\n",
    "            json.dump(doc, f)\n",
    "            \n",
    "    def save(self, docs):\n",
    "        for doc in docs:\n",
    "            self.save_one(doc)\n",
    "            \n",
    "class SpacySuggester:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        spacy_doc = self.model(doc[\"text\"])\n",
    "        return {\n",
    "            **doc,\n",
    "            \"entities\": [\n",
    "                span\n",
    "                for i, ent in enumerate(spacy_doc.ents)\n",
    "                for span in (\n",
    "                    {\n",
    "                        \"id\": f\"metanno-{doc['id']}-{i}\",\n",
    "                        \"begin\": ent.start_char,\n",
    "                        \"end\": ent.end_char,\n",
    "                        \"label\": ent.label_,\n",
    "                        \"negation\": ent._.negation,\n",
    "                        \"suggestions\": {\n",
    "                            \"concept\": [\"C1234\"],\n",
    "                            \"label\": [],\n",
    "                        },\n",
    "                        \"scope\": f\"{i}-scope\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"id\": f\"{i}-scope\",\n",
    "                        \"begin\": ent.start_char,\n",
    "                        \"end\": ent.end_char + 50,\n",
    "                        \"label\": \"scope\",\n",
    "                    }\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from typing import Any, Dict, Iterator, List, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from loguru import logger\n",
    "from spacy import Language\n",
    "from spacy.tokens import Doc, Span\n",
    "from spacy.util import filter_spans\n",
    "from tqdm import tqdm\n",
    "\n",
    "REGEX_ENTITY = re.compile(r'^(T\\d+)\\t([^\\s]+)([^\\t]+)\\t(.*)$')\n",
    "REGEX_NOTE = re.compile(r'^(#\\d+)\\tAnnotatorNotes ([^\\t]+)\\t(.*)$')\n",
    "REGEX_STATUS = re.compile(r'^(#\\d+)\\tStatus ([^\\t]+)\\t(.*)$')\n",
    "REGEX_RELATION = re.compile(r'^(R\\d+)\\t([^\\s]+) Arg1:([^\\s]+) Arg2:([^\\s]+)')\n",
    "REGEX_ATTRIBUTE = re.compile(r'^([AM]\\d+)\\t(.+)$')\n",
    "REGEX_EVENT = re.compile(r'^(E\\d+)\\t(.+)$')\n",
    "REGEX_EVENT_PART = re.compile(r'([^\\s]+):([TE]\\d+)')\n",
    "\n",
    "\n",
    "def load_from_brat(path: str, merge_spaced_fragments: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Load a brat file\n",
    "\n",
    "    Adapted from https://github.com/percevalw/nlstruct/blob/master/nlstruct/datasets/brat.py\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        Path or glob path of the brat text file (.txt, not .ann)\n",
    "    merge_spaced_fragments: bool\n",
    "        Merge fragments of a entity that was splitted by brat because it overlapped an end of line\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Iterator[Dict]\n",
    "    \"\"\"\n",
    "    ann_filenames = []\n",
    "    for filename in glob.glob(str(path).replace(\".txt\", \".a*\"), recursive=True):\n",
    "        ann_filenames.append(filename)\n",
    "\n",
    "    entities = {}\n",
    "    relations = []\n",
    "    events = {}\n",
    "\n",
    "    with open(path) as f:\n",
    "        text = f.read()\n",
    "\n",
    "    note_id = path#.split(\"/\")[-1].rsplit(\".\", 1)[0]\n",
    "\n",
    "    if not len(ann_filenames):\n",
    "        return {\n",
    "            \"id\": note_id,\n",
    "            \"text\": text,\n",
    "            \"entities\": [],\n",
    "        }\n",
    "\n",
    "    doc = {\n",
    "        \"id\": note_id,\n",
    "        \"text\": text,\n",
    "    }\n",
    "    \n",
    "    for ann_file in ann_filenames:\n",
    "        with open(ann_file) as f:\n",
    "            for line_idx, line in enumerate(f):\n",
    "                try:\n",
    "                    if line.startswith('T'):\n",
    "                        match = REGEX_ENTITY.match(line)\n",
    "                        if match is None:\n",
    "                            raise ValueError(f'File {ann_file}, unrecognized Brat line {line}')\n",
    "                        ann_id = match.group(1)\n",
    "                        entity = match.group(2)\n",
    "                        span = match.group(3)\n",
    "                        mention_text = match.group(4)\n",
    "                        entities[ann_id] = {\n",
    "                            \"id\": ann_id,\n",
    "                            \"label\": entity,\n",
    "                            \"fragments\": [],\n",
    "                            \"attributes\": [],\n",
    "                            \"comments\": [],\n",
    "                        }\n",
    "                        last_end = None\n",
    "                        fragment_i = 0\n",
    "                        begins_ends = sorted([(int(s.split()[0]), int(s.split()[1])) for s in span.split(';')])\n",
    "\n",
    "                        for begin, end in begins_ends:\n",
    "                            # If merge_spaced_fragments, merge two fragments that are only separated by a newline (brat automatically creates\n",
    "                            # multiple fragments for a entity that spans over more than one line)\n",
    "                            if merge_spaced_fragments and last_end is not None and len(text[last_end:begin].strip()) == 0:\n",
    "                                entities[ann_id][\"fragments\"][-1][\"end\"] = end\n",
    "                                continue\n",
    "                            entities[ann_id][\"begin\"] = begin\n",
    "                            entities[ann_id][\"end\"] = end\n",
    "                            entities[ann_id][\"fragments\"].append({\n",
    "                                \"begin\": begin,\n",
    "                                \"end\": end,\n",
    "                            })\n",
    "                            fragment_i += 1\n",
    "                            last_end = end\n",
    "                    elif line.startswith('A') or line.startswith('M'):\n",
    "                        match = REGEX_ATTRIBUTE.match(line)\n",
    "                        if match is None:\n",
    "                            raise ValueError(f'File {ann_file}, unrecognized Brat line {line}')\n",
    "                        ann_id = match.group(1)\n",
    "                        parts = match.group(2).split(\" \")\n",
    "                        if len(parts) >= 3:\n",
    "                            entity, entity_id, value = parts\n",
    "                        elif len(parts) == 2:\n",
    "                            entity, entity_id = parts\n",
    "                            value = None\n",
    "                        else:\n",
    "                            raise ValueError(f'File {ann_file}, unrecognized Brat line {line}')\n",
    "                        (entities[entity_id] if entity_id.startswith('T') else events[entity_id])[\"attributes\"].append({\n",
    "                            \"label\": entity,\n",
    "                            \"value\": value,\n",
    "                        })\n",
    "                    elif line.startswith('R'):\n",
    "                        match = REGEX_RELATION.match(line)\n",
    "                        if match is None:\n",
    "                            raise ValueError(f'File {ann_file}, unrecognized Brat line {line}')\n",
    "                        ann_id = match.group(1)\n",
    "                        ann_name = match.group(2)\n",
    "                        arg1 = match.group(3)\n",
    "                        arg2 = match.group(4)\n",
    "                        relations.append({\n",
    "                            \"id\": ann_id,\n",
    "                            \"relation_label\": ann_name,\n",
    "                            \"from_id\": arg1,\n",
    "                            \"to_id\": arg2,\n",
    "                        })\n",
    "                    elif line.startswith('E'):\n",
    "                        match = REGEX_EVENT.match(line)\n",
    "                        if match is None:\n",
    "                            raise ValueError(f'File {ann_file}, unrecognized Brat line {line}')\n",
    "                        ann_id = match.group(1)\n",
    "                        arguments_txt = match.group(2)\n",
    "                        arguments = []\n",
    "                        for argument in REGEX_EVENT_PART.finditer(arguments_txt):\n",
    "                            arguments.append({\"entity_id\": argument.group(2), \"label\": argument.group(1)})\n",
    "                        events[ann_id] = {\n",
    "                            \"id\": ann_id,\n",
    "                            \"attributes\": [],\n",
    "                            \"arguments\": arguments,\n",
    "                        }\n",
    "                    elif line.startswith('#'):\n",
    "                        match = REGEX_STATUS.match(line)\n",
    "                        if match:\n",
    "                            comment = match.group(3)\n",
    "                            doc[\"seen\"] = comment == \"CHECKED\"\n",
    "                            continue\n",
    "                        \n",
    "                        match = REGEX_NOTE.match(line)\n",
    "                        if match is None:\n",
    "                            raise ValueError(f'File {ann_file}, unrecognized Brat line {line}')\n",
    "                        ann_id = match.group(1)\n",
    "                        entity_id = match.group(2)\n",
    "                        comment = match.group(3)\n",
    "                        entities[entity_id][\"comments\"].append({\n",
    "                            \"comment\": comment,\n",
    "                        })\n",
    "                except:\n",
    "                    raise Exception(\"Could not parse line {} from {}: {}\".format(line_idx, filename.replace(\".txt\", \".ann\"), repr(line)))\n",
    "                    \n",
    "    doc.update({\n",
    "        \"entities\": list(entities.values()),\n",
    "        \"relations\": relations,\n",
    "        \"events\": list(events.values()),\n",
    "    })\n",
    "    return doc\n",
    "\n",
    "\n",
    "def export_to_brat(doc, txt_filename, overwrite_txt=False, overwrite_ann=False):\n",
    "    txt_filename = str(txt_filename)\n",
    "    parent_dir = txt_filename.rsplit(\"/\", 1)[0]\n",
    "    if parent_dir and not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir, exist_ok=True)\n",
    "    if not os.path.exists(txt_filename) or overwrite_txt:\n",
    "        with open(txt_filename, \"w\") as f:\n",
    "            f.write(doc[\"text\"])\n",
    "\n",
    "    ann_filename = txt_filename.replace(\".txt\", \".ann\")\n",
    "    attribute_idx = 1\n",
    "    entities_ids = defaultdict(lambda: \"T\" + str(len(entities_ids) + 1))\n",
    "    if not os.path.exists(ann_filename) or overwrite_ann:\n",
    "        with open(ann_filename, \"w\") as f:\n",
    "            if \"entities\" in doc:\n",
    "                for entity in doc[\"entities\"]:\n",
    "                    idx = None\n",
    "                    spans = []\n",
    "                    brat_entity_id = entities_ids[entity[\"id\"]]\n",
    "                    if \"begin\" in entity and \"end\" in entity:\n",
    "                        entity[\"fragments\"] = [{\n",
    "                            \"begin\": entity[\"begin\"],\n",
    "                            \"end\": entity[\"end\"],\n",
    "                        }]\n",
    "                    for fragment in sorted(entity[\"fragments\"], key=lambda frag: frag[\"begin\"]):\n",
    "                        idx = fragment[\"begin\"]\n",
    "                        entity_text = doc[\"text\"][fragment[\"begin\"]:fragment[\"end\"]]\n",
    "                        for part in entity_text.split(\"\\n\"):\n",
    "                            begin = idx\n",
    "                            end = idx + len(part)\n",
    "                            idx = end + 1\n",
    "                            if begin != end:\n",
    "                                spans.append((begin, end))\n",
    "                    print(\"{}\\t{} {}\\t{}\".format(\n",
    "                        brat_entity_id,\n",
    "                        str(entity[\"label\"]),\n",
    "                        \";\".join(\" \".join(map(str, span)) for span in spans),\n",
    "                        entity_text.replace(\"\\n\", \" \")), file=f)\n",
    "                    if \"attributes\" in entity:\n",
    "                        for i, attribute in enumerate(entity[\"attributes\"]):\n",
    "                            if \"value\" in attribute and attribute[\"value\"] is not None and attribute[\"value\"] != \"\":\n",
    "                                print(\"A{}\\t{} {} {}\".format(\n",
    "                                    attribute_idx,\n",
    "                                    str(attribute[\"label\"]),\n",
    "                                    brat_entity_id,\n",
    "                                    attribute[\"value\"]), file=f)\n",
    "                            elif attribute[\"value\"] is True:\n",
    "                                print(\"A{}\\t{} {}\".format(\n",
    "                                    attribute_idx,\n",
    "                                    str(attribute[\"label\"]),\n",
    "                                    brat_entity_id), file=f)\n",
    "                            attribute_idx += 1\n",
    "            if \"relations\" in doc:\n",
    "                for i, relation in enumerate(doc[\"relations\"]):\n",
    "                    entity_from = entities_ids[relation[\"from_id\"]]\n",
    "                    entity_to = entities_ids[relation[\"to_id\"]]\n",
    "                    print(\"R{}\\t{} Arg1:{} Arg2:{}\\t\".format(\n",
    "                        i + 1,\n",
    "                        str(relation[\"label\"]),\n",
    "                        entity_from,\n",
    "                        entity_to), file=f)\n",
    "\n",
    "\n",
    "class BratDataConnector:\n",
    "    # TODO, fix this with Proxy states\n",
    "    def __init__(self, path):\n",
    "        self.path = pathlib.Path(path)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "            \n",
    "    def load(self):\n",
    "        filenames = list(self.path.rglob(\"*.txt\"))\n",
    "        docs = []\n",
    "        for filename in sorted(filenames):\n",
    "            if \".ipynb_checkpoints\" in str(filename):\n",
    "                continue\n",
    "            filename = pathlib.Path(filename).relative_to(self.path)\n",
    "            doc = load_from_brat(str(self.path.joinpath(filename)))\n",
    "            doc[\"id\"] = str(filename)\n",
    "            if doc is not None:\n",
    "                docs.append(doc)\n",
    "        return docs\n",
    "    \n",
    "    def save_one(self, doc):\n",
    "        export_to_brat(doc, self.path.joinpath(doc['id']), overwrite_ann=True)\n",
    "        # with open(self.path.joinpath(f\"{doc['id']}.json\"), \"w\") as f:\n",
    "        #     json.dump(doc, f)\n",
    "            \n",
    "    def save(self, docs):\n",
    "        for doc in docs:\n",
    "            self.save_one(doc)\n",
    "\n",
    "suggester = SpacySuggester(nlp)\n",
    "data = BratDataConnector(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790ad6ef-3a84-4ac8-9d6e-5e00e1b9721d",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Counter\n\u001B[0;32m----> 2\u001B[0m labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(\u001B[38;5;28mset\u001B[39m([ent[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m \u001B[43mdata\u001B[49m\u001B[38;5;241m.\u001B[39mload() \u001B[38;5;28;01mfor\u001B[39;00m ent \u001B[38;5;129;01min\u001B[39;00m doc[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mentities\u001B[39m\u001B[38;5;124m\"\u001B[39m]]))\n\u001B[1;32m      3\u001B[0m keys \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m labels:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "labels = sorted(set([ent[\"label\"] for doc in data.load() for ent in doc[\"entities\"]]))\n",
    "keys = {}\n",
    "for label in labels:\n",
    "    keys[label] = next(letter for letter in label.lower() if letter not in keys.values())\n",
    "app = NERApp(\n",
    "    data=data,\n",
    "    suggester=suggester,\n",
    "    scheme={\n",
    "        \"labels\": [\n",
    "            {\"name\": label, \"color\": colors[i], \"key\": keys[label]}\n",
    "            for i, label in enumerate(labels)\n",
    "        ],\n",
    "        \"attributes\": [{\n",
    "            \"name\": \"modality\",\n",
    "            \"kind\": \"text\",\n",
    "            \"key\": \"m\",\n",
    "            \"color\": \"lightgrey\",\n",
    "            \"choices\": [\"factual\", \"negated\", \"conditional\", \"counterindication\", \"uncertain\", \"suggested\"]\n",
    "        }, {\n",
    "            \"name\": \"experiencer\",\n",
    "            \"kind\": \"text\",\n",
    "            \"key\": \"x\",\n",
    "            \"color\": \"lightgrey\",\n",
    "            \"choices\": [\"self\", \"family\", \"other\"],\n",
    "        }, {\n",
    "            \"name\": \"time\",\n",
    "            \"kind\": \"text\",\n",
    "            \"key\": \"t\",\n",
    "            \"color\": \"lightgrey\",\n",
    "            \"choices\": [\"present\", \"past\", \"future\"],\n",
    "        }, {\n",
    "            \"name\": \"concept\",\n",
    "            \"kind\": \"text\",\n",
    "            \"color\": \"lightgrey\",\n",
    "            \"choices\": [f\"C{n:04}\" for n in range(10000)],\n",
    "        }],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fefb8891-ed69-4263-86f3-4e61ce58bab9",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcrypt (TM) Python to JavaScript Small Sane Subset Transpiler Version 3.9.0\n",
      "Copyright (C) Geatec Engineering. License: Apache 2.0\n",
      "\n",
      "\n",
      "Saving target code in: /Users/perceval/Development/metanno/examples/__target__/test.js\n",
      "\n"
     ]
    }
   ],
   "source": [
    "app = NERApp(\n",
    "    data=data,\n",
    "    suggester=suggester,\n",
    "    scheme={\n",
    "        \"labels\": [{\n",
    "            \"name\": \"nom\",\n",
    "            \"key\": \"a\",\n",
    "            \"color\": colors[3],\n",
    "        }, {\n",
    "            \"name\": \"covid\",\n",
    "            \"key\": \"c\",\n",
    "            \"color\": colors[1],\n",
    "        }, {\n",
    "            \"name\": \"custom\",\n",
    "            \"key\": \"u\",\n",
    "            \"color\": colors[2],\n",
    "        }, {\n",
    "            \"name\": \"eds.emergency.priority\",\n",
    "            \"key\": \"e\",\n",
    "            \"color\": colors[0],\n",
    "        },\n",
    "        #    {\n",
    "        #    \"name\": \"scope\",\n",
    "        #    \"key\": \"\",\n",
    "        #    \"color\": \"#ffedfa\",\n",
    "        #    \"border\": \"#ffedfa\",\n",
    "        #    \"alpha\": 1.0,\n",
    "        #    \"shape\": \"fullHeight\",\n",
    "        #}\n",
    "        ],\n",
    "        \"attributes\": [],\n",
    "        #{\n",
    "        #    \"name\": \"modality\",\n",
    "        #    \"kind\": \"text\",\n",
    "        #    \"key\": \"m\",\n",
    "        #    \"color\": \"lightgrey\",\n",
    "        #    \"choices\": [\"factual\", \"negated\", \"conditional\", \"counterindication\", \"uncertain\", \"suggested\"]\n",
    "        #}, {\n",
    "        #    \"name\": \"experiencer\",\n",
    "        #    \"kind\": \"text\",\n",
    "        #    \"key\": \"x\",\n",
    "        #    \"color\": \"lightgrey\",\n",
    "        #    \"choices\": [\"self\", \"family\", \"other\"],\n",
    "        #}, {\n",
    "        #    \"name\": \"time\",\n",
    "        #    \"kind\": \"text\",\n",
    "        #    \"key\": \"t\",\n",
    "        #    \"color\": \"lightgrey\",\n",
    "        #    \"choices\": [\"present\", \"past\", \"future\"],\n",
    "        #}, {\n",
    "        #    \"name\": \"concept\",\n",
    "        #    \"kind\": \"text\",\n",
    "        #    \"color\": \"lightgrey\",\n",
    "        #    \"choices\": [f\"C{n:04}\" for n in range(10000)],\n",
    "        #}],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42d959e5-8b2b-493c-9470-cd2ee12ddf0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from metanno import manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b40dc59b-00d4-497e-93d3-1b29d923652b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metanno.manager import AppManager\n",
    "AppManager()._state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52cb39-5e20-4792-8103-3ac971487ffe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('test.json', 'w') as f:\n",
    "    json.dump(app.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4437a71-8643-4125-a246-be2797716fda",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.annotator+json": {
       "editor-id": "text",
       "editor-type": "span-editor",
       "version_major": 0,
       "version_minor": 0
      },
      "text/plain": [
       "<metanno.views.SpanEditor object at 0x7fe84aa1bd60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.span_editor(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abbf6613-6c60-45a5-8d4b-c8b5e7e698e6",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res = app.table_editor(\"docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b58cb913-a4a9-46fa-8862-8ac979ac0042",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.annotator+json": {
       "editor-id": "docs",
       "editor-type": "table-editor",
       "version_major": 0,
       "version_minor": 0
      },
      "text/plain": [
       "<metanno.views.TableEditor object at 0x7f9066189790>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26926bbe-1fd1-464a-bec7-abd34e36ef3f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.annotator+json": {
       "editor-id": "entities",
       "editor-type": "table-editor",
       "version_major": 0,
       "version_minor": 0
      },
      "text/plain": [
       "<metanno.views.TableEditor object at 0x7f90ad4806a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.table_editor(\"entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e29304e6-98e6-4ffb-9e43-e673fcb6a0f8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'id': 0,\n",
       "  'begin': 49,\n",
       "  'end': 74,\n",
       "  'label': 'custom',\n",
       "  'negation': False,\n",
       "  'suggestions': {'concept': ['C1234'], 'label': []},\n",
       "  'scope': '0-scope'},\n",
       " '0-scope': {'id': '0-scope', 'begin': 49, 'end': 124, 'label': 'scope'},\n",
       " 1: {'id': 1,\n",
       "  'begin': 130,\n",
       "  'end': 141,\n",
       "  'label': 'custom',\n",
       "  'negation': True,\n",
       "  'suggestions': {'concept': ['C1234'], 'label': []},\n",
       "  'scope': '1-scope'},\n",
       " '1-scope': {'id': '1-scope', 'begin': 130, 'end': 191, 'label': 'scope'},\n",
       " 2: {'id': 2,\n",
       "  'begin': 342,\n",
       "  'end': 350,\n",
       "  'label': 'eds.emergency.priority',\n",
       "  'negation': False,\n",
       "  'suggestions': {'concept': ['C1234'], 'label': []},\n",
       "  'scope': '2-scope'},\n",
       " '2-scope': {'id': '2-scope', 'begin': 342, 'end': 400, 'label': 'scope'},\n",
       " 3: {'id': 3,\n",
       "  'begin': 406,\n",
       "  'end': 430,\n",
       "  'label': 'covid',\n",
       "  'negation': False,\n",
       "  'suggestions': {'concept': ['C1234'], 'label': []},\n",
       "  'scope': '3-scope'},\n",
       " '3-scope': {'id': '3-scope', 'begin': 406, 'end': 480, 'label': 'scope'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.state[\"docs\"][\"doc1-Copy1.txt\"][\"entities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c43cac-c600-4890-adb0-c98ce3505bba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edsnlp",
   "language": "python",
   "name": "edsnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}